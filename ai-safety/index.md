---
title:
    AI safety reading group
description:
    preventing civilisation from destroying itself
---

AI safety reading group
=======================

Weekly discussions of readings on technical and philosophical topics in
AI safety.

AI Safety is the field trying to figure out how to stop AI systems from
breaking the world, and in particular, trying to do so before they break
the world.
Readings will span from potential issues arising from future advanced
AI systems, to technical topics in AI control, to present-day issues.

Seminar information:

* **Organisers:**
  Matthew Farrugia-Roberts and Dan Murfet.
* **Time:**
  Thursday evenings, 9pm AEDT (UTC +11), most weeks
  (see [home page](/) for most up-to-date schedule).
* **Venue:**
  [The Rising Sea](https://www.roblox.com/games/8165217582/The-Rising-Sea).

Directions for joining discussions:

0. New to metauni?
   Follow [these instructions (part 2)](/posts/instructions/instructions)
   to join the metauni Discord server, and introduce yourself in the channel
   `#ai-safety`.
1. Metauni talks take place in Roblox using in-game voice chat.
   Follow [these instructions (part 1)](/posts/instructions/instructions)
   to create a Roblox account, complete "age verification" (unfortunately,
   this involves sharing ID with Roblox), and then enable Roblox "voice chat".
2. At the scheduled discussion time, launch the Roblox experience
   [The Rising Sea](https://www.roblox.com/games/8165217582/The-Rising-Sea)
   and then step into matomatical's portal (bottom-right corner of stack, see
   picture),
   or use the menu: "Pockets" > "Go to pocket" > type address "Gemini Pulsar 1".
   ![](map.jpg)

Readings
--------

Completing weekly readings is recommended, but ultimately optional.
The discussion sessions begin with a summary of the reading, lead by Matt
(unless otherwise noted).

Upcoming readings and discussions:

* **2022.10.06:**
  Eliezer Yudkowsky,
  2013,
  "Intelligence explosion microeconomics",
  MIRI [technical report](https://intelligence.org/files/IEM.pdf).

Past readings and discussions:

* **2022.06.09:**
  Norbert Wiener,
  1960,
  "Some moral and technical consequences of automation",
  *Science*.

* **2022.06.16:**
  Stephen M. Omohundro,
  2008,
  "The basic AI drives",
  *Proceedings of the 2008 conference on Artificial General Intelligence*.

* **2022.06.23:**
  Nick Bostrom,
  2012,
  "The superintelligent will: Motivation and instrumental rationality in
  advanced artificial agents",
  *Minds and Machines*.

* **2022.06.30:**
  Rachel Thomas and Louisa Bartolo,
  2022,
  "AI harms are societal, not just individual",
  [fast.ai blog](https://www.fast.ai/2022/05/17/societal-harms/).
  Discussion lead by Dan.

* **2022.07.21:**
  Tobias WÃ¤ngberg *et al.*,
  2017,
  "A game-theoretic analysis of the of the off-switch game",
  *AGI 2017*.

* **2022.07.28:**
  Abram Demski and Scott Garrabrant,
  2019,
  "Embedded agency",
  [arXiv](https://arxiv.org/abs/1902.09469)
  / [sequence](https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh).

* **2022.08.04:**
  Scott Garrabrant *et al.*,
  2017,
  "Logical induction",
  [arXiv](https://arxiv.org/abs/1609.03543v4).
  Discussion lead by Dan.
  Note: there is an updated 2020 version on arXiv.

* **2022.08.11:**
  Robin Hanson,
  2022,
  "Why not wait on AI risk?",
  [overcoming bias blog](https://www.overcomingbias.com/2022/06/why-not-wait-on-ai-risk.html)
  and
  "Foom update",
  [overcoming bias blog](https://www.overcomingbias.com/2022/05/foom-update.html).

* **2022.08.18:**
  Evan Hubinger *et al.*,
  2019,
  "Risks from learned optimization"
  [arXiv](https://arxiv.org/abs/1906.01820)
  / [sequence](https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB).

* **2022.08.25:**
  an original presentation by Matt about compression and learning in models
  of computation embedded in the real world.

* **2022.09.08:**
  discussion of reading group direction.

* **2022.09.15:**
  Nate Soares,
  2022,
  "On how various plans miss the hard bits of the alignment challenge",
  [lesswrong](https://www.lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment).

Topics brainstorm
-----------------

AI safety is political philosophy complete

* Exernalities correspond to market alignment failures. How do we handle
  them? Overcome them? Do we face risk from them? Would these risks be
  exacerbated by 
* How can we live in the midst of complex systems we don't understand, and
  can't fully control, like civilisation, capitalism, etc.?
* What other literatures could help us here?

Other topics

* AI governance
* Is there literature on technology and society?
* Specific safety proposals

Sources of readings (clearly with much mutual overlap):

* Matt's lists (TODO: share them).
* Victoria Krakovna's
  [resource](https://vkrakovna.wordpress.com/2016/02/28/introductory-resources-on-ai-safety-research/)
  [lists](https://vkrakovna.wordpress.com/ai-safety-resources/).
* Rohin Shah's
  [2018/2019 review](https://www.alignmentforum.org/posts/dKxX76SCfCvceJXHv/ai-alignment-2018-19-review).
* CHAI AI safety [bibliography](https://humancompatible.ai/bibliography)
* Publications from
  [MIRI](https://intelligence.org/research/#publications),
  [FHI](http://www.fhi.ox.ac.uk/publications/), etc.
* The old 80kh AI safety
  [syllabus](https://80000hours.org/articles/ai-safety-syllabus/)
  and links therein (esp. EA Cambridge syllabus).
