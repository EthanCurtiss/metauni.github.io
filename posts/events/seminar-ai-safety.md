---
title:
    AI safety reading group
description:
    preventing civilisation from destroying itself
---

AI safety reading group
=======================

Weekly discussions of readings on technical and philosophical topics in
AI safety.

AI Safety is the field trying to figure out how to stop AI systems from
breaking the world, and in particular, trying to do so before they break
the world.
Readings will mostly focus on potential issues arising from future advanced
AI systems, and may also touch on present-day issues.

* **Organisers:**
  Matthew Farrugia-Roberts and Dan Murfet.
* **Venue:**:
  [The Rising Sea](https://www.roblox.com/games/8165217582/The-Rising-Sea).
  ![Step into matomatical's portal (bottom-right corner of stack), or
  use the menu: "Pockets" > "Go to pocket" > type address "Gemini Pulsar 1"
  ](seminar-ai-safety-map.jpg)
* **Time:**
  Thursday evenings, 9pm AEST (see [home page](/) for updated schedule).

New to metauni? Follow [these instructions](/posts/instructions/instruction)
to get started.

Readings
--------

Completing weekly readings is recommended, but ultimately optional.
The discussion sessions begin with a summary of the reading, lead by Matt
(unless otherwise noted).

Scheduled readings:

* **2022.06.09:**
  Norbert Wiener, 1960, "Some moral and technical consequences of
  automation", *Science*.
  DOI:[10.1126/science.131.3410.1355](https://doi.org/10.1126/science.131.3410.1355).

* **2022.06.16:** TBD

Topics brainstorm
-----------------

The nature of superintelligence

* On the instrumental convergence thesis (Omohundro, 2008; Bostrom, 2014)
* On the orthogonality thesis
* Key chapters of Bostrom's *Superintelligence*
* On embedded agency (Demski & Garrabrant, 2020)
* Eric Drexler's report *Reframing Superintelligence*
  / Comprehensive AI Services (CAIS)

Aligning superintelligences:

* Key chapters of Stuart Russell's *Human Compatible*
* Key chapters of Brian Christian's *The Alignment Problem*
* Papers on CIRL / assistance games
* Papers on corrigibility
* Papers on the off-switch game
* Papers on algorithmic bias
* Papers on interpretability
* Papers on the complexity of values thesis
* Papers on mesa optimisation / optimisation daemons

Present-day issues:

* TODO

Sources of readings (clearly with much mutual overlap):

* Matt's lists (TODO).
* Victoria Krakovna's
  [resource](https://vkrakovna.wordpress.com/2016/02/28/introductory-resources-on-ai-safety-research/).
  [lists](https://vkrakovna.wordpress.com/ai-safety-resources/)
* Rohin Shah's
  [2018/2019 review](https://www.alignmentforum.org/posts/dKxX76SCfCvceJXHv/ai-alignment-2018-19-review).
* CHAI AI safety [bibliography](https://humancompatible.ai/bibliography)
* Publications from
  [MIRI](https://intelligence.org/research/#publications),
  [FHI](http://www.fhi.ox.ac.uk/publications/), etc.
* The old 80kh AI safety
  [syllabus](https://80000hours.org/articles/ai-safety-syllabus/)
  and links therein (esp. EA Cambridge syllabus).
